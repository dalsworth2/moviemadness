---
title: "GLM Regression"
author: "Maria Dhaliwal"
date: "28/09/2021"
output: 
  html_document:
    toc: true
    toc_depth: 3
    toc_float: true
    number_sections: false
    final_ramen_print: kable
    theme: spacelab
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Resources

## Load Packages

Load some libraries first for this regression.

```{r libraries, warning=FALSE}
suppressMessages(library(readr))
suppressMessages(library(dplyr))
suppressMessages(library(ggplot2))
suppressMessages(library(tidyverse))
suppressMessages(library(stringr))
suppressMessages(library(statsr))
suppressMessages(library(gridExtra))
suppressMessages(library(corrplot))
suppressMessages(library(summarytools))
suppressMessages(library(leaps))
suppressMessages(library(tictoc))
suppressMessages(library(bestglm))
suppressMessages(library(caret))

```

## Load Data

Load the API-acquired datasets

```{r tmdb, warning=FALSE}

tmdb <- read_tsv("../data/all_top_rated_movies_2021-09-01.tsv")

data(tmdb)
str(tmdb)

```

# Data

## Explore dataset

```{r explore, warning=FALSE}

colSums(is.na(tmdb)) #check for missing values
glimpse(tmdb)
summary(tmdb)
dfSummary(tmdb, graph.magnif = 0.75, valid.col = FALSE, style = "grid")

```

## Data Cleansing

Removal of records containing NA as per EDA.

```{r cleanse, message=FALSE, warning=FALSE}

#Removal of the rows with the 16 missing values in the column overview, 2 missing values in genre ID, and .
tmdb1 =  na.omit(tmdb)

length(unique(tmdb1$title))

# Returns the unique rows in movies dataset
tmdb1 %>% distinct()

# Remove duplicate rows based on multiple variables 
tmdb1 %>% distinct(title, release_date, .keep_all = TRUE)

```

## Add new variable

Adding a new variable in for the GLM regression. Assuming that a high vote_average is what classifies as popular and high performing movie, then we will separate out any vote_average that is greater than or equal to 7. 

```{r new-factor, warning=FALSE}

# Greater than 8 is set to 1 which represents a high performance, popular movie
# Everything else is 0 
tmdb1$vote_rate = ifelse(tmdb1$vote_average >= 8, 1, 0)

```


# Regression

Based on the QQ plots that resulted from our exploratory data analysis, we can see that some factors are normally distributed as some of the data points are fitted for the line, and most are right skewed. Using a generalized linear model will allow for a more relaxed approach and be able to account for a variety of distributions.

## Test/Train Split

Partition data for training (70%) and testing (30%). Whilst doing so, set a random seed to ensure we get the same result each time.

```{r partition, warning=FALSE}

set.seed(42)

#Create the sample size for regression
training_size <- floor(0.70 * nrow(tmdb1))

#Set indices for the dataset
trainingset_indices <- sample(seq_len(nrow(tmdb1)), size = training_size)

#Assign observations to training and testing sets
trainingset <- tmdb1[trainingset_indices, ]
testingset <- tmdb1[-trainingset_indices, ]

#rowcounts to check
nrow(trainingset)
nrow(testingset)
nrow(tmdb1)

```

## First Generalized linear model (GLM) Regression

So back to our research question: "How can production studios and streaming platforms improve the performance and popularity of their movies?" 

### Variable Selection

For this model, based on the previous codes, we will utilise the same factors:
revenue, popularity, budget, vote_count, runtime, and the genres

```{r variable-selection, warning=FALSE}

tmdb_glm = glm(formula = vote_rate ~ revenue + popularity + budget + vote_count + runtime + Action + Adventure + Animation + Comedy + Crime + Documentary + Drama + Family + Fantasy + History + Horror + Music + Mystery + Romance + `Science Fiction` + `TV Movie` + Thriller + War + Western,
               data = trainingset,
               family = binomial(logit)
                 )
```


```{r plots-et-al, warning=FALSE}
summary(tmdb_glm)

plot(tmdb_glm)

ggplot(data = tmdb_glm, aes(x = .fitted, y = .resid)) +
  geom_point() +
  geom_hline(yintercept = 0, linetype = "dashed") +
  xlab("Fitted values") +
  ylab("Residuals")


#Showing the residuals
ggplot(data = tmdb_glm, aes(x = .resid)) +
  geom_histogram(binwidth = 1, fill='white', color='black') +
  xlab("Residuals")

imp <- as.data.frame(varImp(tmdb_glm))
imp <- data.frame(overall = imp$Overall,
           names   = rownames(imp))
imp[order(imp$overall,decreasing = T),]

```

Based on the summary, we can see that the following factors play a big importance:
popularity, budget, vote_count, runtime, Animation, Drama, Horror, Science Fiction, Thriller, and Western.

### First Model Confusion Matrix

Let's have a look at this model's evaluation first:

```{r cfm, warning=FALSE}
# Predict probabilities on the testing set
glm_prob <- predict(tmdb_glm, testingset[,-1], type = "response")

# Create a vector to hold predictions
glm_predict <- rep(0,nrow(testingset[,-1]))
glm_predict[glm_prob >= 0.5] <- 1

# Create a confusion matrix
glm_confusion_matrix <- table(pred = glm_predict, 
                             true = testingset$vote_rate)

glm_confusion_matrix

```

### First Model Evaluation

```{r eval, warning=FALSE}
get_evaluation_measurements1 <- function(name = NA, tn, fp, fn, tp) {
  
  accuracy = (tp+tn)/(tp+tn+fp+fn)
  
  precision = tp/(tp+fp)
  
  recall = tp/(tp+fn)
  
  F1 = 2 * ((precision * recall)/(precision + recall))
  
  output = data.frame(name, accuracy, precision, recall, F1)
  
  return(output)
  
}


# Data frame the confusion matrix result and output the evaluation measures

glm_confusion_matrix_df <- as.data.frame(glm_confusion_matrix)

glm_evaluation_measures1 <- get_evaluation_measurements1("GLM1",
                                glm_confusion_matrix_df$Freq[1],
                                glm_confusion_matrix_df$Freq[2],
                                glm_confusion_matrix_df$Freq[3],
                                glm_confusion_matrix_df$Freq[4])

glm_evaluation_measures1
```


## Second GLM - less variables


```{r glm2, warning=FALSE}

tmdb_glm2 = glm(formula = vote_rate ~ popularity + budget + vote_count + runtime + Animation + Drama + Horror + `Science Fiction` + Thriller + Western,
               data = trainingset,
               family = binomial(logit)
                 )

```

```{r plots-summary, warning=FALSE}

summary(tmdb_glm2)

plot(tmdb_glm2)

ggplot(data = tmdb_glm2, aes(x = .fitted, y = .resid)) +
  geom_point() +
  geom_hline(yintercept = 0, linetype = "dashed") +
  xlab("Fitted values") +
  ylab("Residuals")


#Showing the residuals
ggplot(data = tmdb_glm2, aes(x = .resid)) +
  geom_histogram(binwidth = 1, fill='white', color='black') +
  xlab("Residuals")


imp2 <- as.data.frame(varImp(tmdb_glm2))
imp2 <- data.frame(overall = imp2$Overall,
           names   = rownames(imp2))
imp2[order(imp2$overall,decreasing = T),]

```


### Second Model Confusion Matrix

Let's have a look at the second model's evaluation:

```{r cfm2, warning=FALSE}
# Predict probabilities on the test set
glm_prob2 <- predict(tmdb_glm2, testingset[,-1], type = "response")

# Create a vector to hold predictions
glm_predict2 <- rep(0,nrow(testingset[,-1]))
glm_predict2[glm_prob2 >= 0.5] <- 1

# Create a confusion matrix
glm_confusion_matrix2 <- table(pred = glm_predict2, 
                             true = testingset$vote_rate)

glm_confusion_matrix2

```


### Second Model Evaluations

```{r eval2, warning=FALSE}

get_evaluation_measurements2 <- function(name = NA, tn, fp, fn, tp) {
  
  accuracy = (tp+tn)/(tp+tn+fp+fn)
  
  precision = tp/(tp+fp)
  
  recall = tp/(tp+fn)
  
  F1 = 2 * ((precision * recall)/(precision + recall))
  
  output2 = data.frame(name, accuracy, precision, recall, F1)
  
  return(output2)
  
}


# Data frame the confusion matrix result and output the evaluation measures

glm_confusion_matrix_df2 <- as.data.frame(glm_confusion_matrix2)

glm_evaluation_measures2 <- get_evaluation_measurements2("GLM2",
                              glm_confusion_matrix_df2$Freq[1],
                              glm_confusion_matrix_df2$Freq[2],
                              glm_confusion_matrix_df2$Freq[3],
                              glm_confusion_matrix_df2$Freq[4])

glm_evaluation_measures2

```

# Findings

By creating a new variable, the GLM model has provided a higher accuracy and higher precision. This shows that there is a high precision in predicting which movie is classified as a high performing and popular movie. It should also be noted that the following coefficients were of high significance to the vote_rate:
 
 - popularity, vote_count, runtime, Animation, and Drama, all of which has a positive influence on vote_rate
 - budget, which has a negative influence on vote_rate

